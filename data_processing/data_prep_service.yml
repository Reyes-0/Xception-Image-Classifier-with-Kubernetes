apiVersion: v1
kind: Service
metadata:
  name: model-inference-service
spec:
  selector:
    app: model-inference
  ports:
  - name: http
    port: 80
    targetPort: 80
  type: LoadBalancer
  # not sure if needed

apiVersion: v1
kind: Service
metadata:
  name: data-prep-service
spec:
  selector:
    app: data-prep
  ports:
    - protocol: TCP
      port: 80
      targetPort: 5000  # Assuming your app is running on port 5000 inside the container


apiVersion: v1
kind: PersistentVolume
metadata:
  name: data-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  hostPath:
    path: "/mnt/data"
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: processed-data-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  hostPath:
    path: "/mnt/processed_data"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: data-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: processed-data-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: data-processing-pod
spec:
  containers:
  - name: data-processing-container
    image: jurnjie/data-processing:latest
    volumeMounts:
    - mountPath: /mnt/data
      name: data-volume
    - mountPath: /mnt/processed_data
      name: processed-data-volume
  volumes:
  - name: data-volume
    persistentVolumeClaim:
      claimName: data-pvc
  - name: processed-data-volume
    persistentVolumeClaim:
      claimName: processed-data-pvc